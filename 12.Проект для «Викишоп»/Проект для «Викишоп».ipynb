{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала загрузим все нужные нам библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import notebook\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statistics as st\n",
    "import math\n",
    "\n",
    "import re\n",
    "from pymystem3 import Mystem\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "import transformers\n",
    "from tqdm import notebook\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откроим фаил и посмотрим общие данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    data = pd.read_csv('C:/Users/Пользователь/OneDrive/Документы/Python/toxic_comments.csv')\n",
    "except:\n",
    "    data = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "\n",
    "display(data.head(3))\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на дисбаланс классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143106\n",
       "1     16186\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    89.84%\n",
       "1    10.16%\n",
       "Name: toxic, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(data['toxic'].value_counts())\n",
    "data['toxic'].value_counts(normalize=True).map('{:.2%}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим на пропуски."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "text          0\n",
       "toxic         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейдем к подготовке признаков и разобъем датасет на выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "doc = nlp(sentence)\n",
    "\n",
    "\" \".join([token.lemma_ for token in doc])\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b98e3e5daf4fbfb38b946c5409b91e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemma_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edit make under my usernam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>d aww he match this background colour I be see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man I be really not try to edit war it be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nmore\\ni can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>more I can not make any real suggestion on imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir be my hero any chance you remember wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>\"\\n\\ncongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>congratulation from I as well use the tool wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>cocksucker before you piss around on my work</td>\n",
       "      <td>1</td>\n",
       "      <td>cocksucker before you piss around on my work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>your vandalism to the matt shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>your vandalism to the matt shirvington article...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>sorry if the word nonsense be offensive to you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>alignment on this subject and which be contrar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic  \\\n",
       "0           0  explanation\\nwhy the edits made under my usern...      0   \n",
       "1           1  d'aww! he matches this background colour i'm s...      0   \n",
       "2           2  hey man, i'm really not trying to edit war. it...      0   \n",
       "3           3  \"\\nmore\\ni can't make any real suggestions on ...      0   \n",
       "4           4  you, sir, are my hero. any chance you remember...      0   \n",
       "5           5  \"\\n\\ncongratulations from me as well, use the ...      0   \n",
       "6           6       cocksucker before you piss around on my work      1   \n",
       "7           7  your vandalism to the matt shirvington article...      0   \n",
       "8           8  sorry if the word 'nonsense' was offensive to ...      0   \n",
       "9           9  alignment on this subject and which are contra...      0   \n",
       "\n",
       "                                          lemma_text  \n",
       "0  explanation why the edit make under my usernam...  \n",
       "1  d aww he match this background colour I be see...  \n",
       "2  hey man I be really not try to edit war it be ...  \n",
       "3  more I can not make any real suggestion on imp...  \n",
       "4  you sir be my hero any chance you remember wha...  \n",
       "5  congratulation from I as well use the tool wel...  \n",
       "6       cocksucker before you piss around on my work  \n",
       "7  your vandalism to the matt shirvington article...  \n",
       "8  sorry if the word nonsense be offensive to you...  \n",
       "9  alignment on this subject and which be contrar...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = data['text'].values\n",
    "data['text'] = data['text'].str.lower()\n",
    "\n",
    "l = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatize_text(phrase):\n",
    "    phrase_l = l(phrase)\n",
    "    lemma_phrase = \" \".join([token.lemma_ for token in phrase_l])\n",
    "    finished_text = re.sub(r'[^a-zA-Z]', ' ', lemma_phrase) \n",
    "    return \" \".join(finished_text.split())\n",
    "\n",
    "data['lemma_text'] = data['text'].progress_apply(lemmatize_text)\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_train shape is (127433, 133938)\n",
      "features_valid shape is (15930, 133938)\n",
      "features_test shape is (15929, 133938)\n"
     ]
    }
   ],
   "source": [
    "features = data['lemma_text']\n",
    "target = data['toxic']\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, \n",
    "                                                                              target, \n",
    "                                                                              test_size=0.2, \n",
    "                                                                              random_state=12345)\n",
    "\n",
    "features_test, features_valid, target_test, target_valid = train_test_split(features_test, \n",
    "                                                                              target_test, \n",
    "                                                                              test_size=0.5, \n",
    "                                                                              random_state=12345)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "\n",
    "features_train = count_tf_idf.fit_transform(features_train.values)\n",
    "features_valid = count_tf_idf.transform(features_valid.values)\n",
    "features_test = count_tf_idf.transform(features_test.values)\n",
    "\n",
    "print(f'features_train shape is {features_train.shape}')\n",
    "print(f'features_valid shape is {features_valid.shape}')\n",
    "print(f'features_test shape is {features_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ПРедварительная работа сделана, перейдем к обучению моделей. Возьмем для сравнения несколько: LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, CatBoostClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.7909527073337903\n"
     ]
    }
   ],
   "source": [
    "model_lr = LogisticRegression(C=10, solver='lbfgs', max_iter=10000, class_weight=1).fit(features_train, target_train)\n",
    "predictions_lr = model_lr.predict(features_valid)\n",
    "f1_score_lr = f1_score(target_valid, predictions_lr)\n",
    "print(f'F1: {f1_score_lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.6736448598130841\n"
     ]
    }
   ],
   "source": [
    "model_dtc = DecisionTreeClassifier(max_depth=25).fit(features_train, target_train)\n",
    "predictions = model_dtc.predict(features_valid)\n",
    "f1_score_dct = f1_score(target_valid, predictions)\n",
    "print(f'F1: {f1_score_dct}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.05045045045045045\n"
     ]
    }
   ],
   "source": [
    "model_rfc = RandomForestClassifier(n_estimators = 60, max_depth=50).fit(features_train, target_train)\n",
    "predictions = model_rfc.predict(features_valid)\n",
    "f1_score_rfc = f1_score(target_valid, predictions)\n",
    "print(f'F1: {f1_score_rfc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cb = CatBoostClassifier(verbose=False, iterations=250).fit(features_train, target_train)\n",
    "predictions = model_cb.predict(features_valid)\n",
    "f1_score_cb_cv = cross_val_score(model_cb,\n",
    "                                         features_train, \n",
    "                                         target_train, \n",
    "                                         cv=3, \n",
    "                                         scoring='f1').mean()\n",
    "f1_score_cb = f1_score(target_valid, predictions)\n",
    "print(f'F1: {f1_score_cb}')\n",
    "print('F1 на кросс-валидации', f1_score_cb_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = pd.DataFrame(data = [round(f1_score_lr, 2), round(f1_score_dct, 2), round(f1_score_rfc, 2), round(f1_score_cb, 2)],\n",
    "                         index=['LogisticRegression', 'DecisionTreeClassifier', 'RandomForestClassifier', 'CatBoostClassifier'],\n",
    "                         columns = ['F1'])\n",
    "f1_scores = f1_scores.sort_values(by='F1', ascending=False)\n",
    "display(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очень неожиданно победителем оказалась модель LogisticRegression!! Такого раньше не было)\n",
    "Не исключено, что при смене показателей гиперпараметров у модели CatBoostClassifier можно достич лучшего результата."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поверим модель на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lr = model_lr.predict(features_test)\n",
    "f1_score_lr = f1_score(target_test, predictions_lr)\n",
    "print(f'F1: {f1_score_lr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все отлично, расхождения минимальны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача выполнена. Мы обработали данный нам датасет, сформировали признаки и протестировали несколько моделей.\n",
    "Показатели метрики F1 выше заданного, а значит хрупкая психика пользователей не достигших 18 лет и просто неравнодушных к родному языку находится под стражей нашего кода))"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 2895,
    "start_time": "2023-06-13T14:28:27.579Z"
   },
   {
    "duration": 389,
    "start_time": "2023-06-13T14:28:30.476Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-13T14:28:30.867Z"
   },
   {
    "duration": 1702,
    "start_time": "2023-06-13T14:28:46.561Z"
   },
   {
    "duration": 10,
    "start_time": "2023-06-13T14:28:50.841Z"
   },
   {
    "duration": 1782,
    "start_time": "2023-06-13T14:29:21.278Z"
   },
   {
    "duration": 1088,
    "start_time": "2023-06-13T14:29:23.062Z"
   },
   {
    "duration": 8003,
    "start_time": "2023-06-13T14:29:24.152Z"
   },
   {
    "duration": 38884,
    "start_time": "2023-06-13T14:33:37.131Z"
   },
   {
    "duration": 1147,
    "start_time": "2023-06-13T14:34:16.022Z"
   },
   {
    "duration": 22,
    "start_time": "2023-06-13T14:34:17.170Z"
   },
   {
    "duration": 105,
    "start_time": "2023-06-13T14:34:17.193Z"
   },
   {
    "duration": 2630,
    "start_time": "2023-06-13T14:34:17.300Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-13T14:34:19.932Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-13T14:34:19.933Z"
   },
   {
    "duration": 5853,
    "start_time": "2023-06-13T14:34:25.015Z"
   },
   {
    "duration": 1103,
    "start_time": "2023-06-13T14:34:30.870Z"
   },
   {
    "duration": 40,
    "start_time": "2023-06-13T14:34:31.974Z"
   },
   {
    "duration": 58,
    "start_time": "2023-06-13T14:34:32.019Z"
   },
   {
    "duration": 805,
    "start_time": "2023-06-13T14:34:32.078Z"
   },
   {
    "duration": 5849,
    "start_time": "2023-06-15T11:31:31.904Z"
   },
   {
    "duration": 2382,
    "start_time": "2023-06-15T11:31:37.755Z"
   },
   {
    "duration": 10,
    "start_time": "2023-06-15T11:31:40.139Z"
   },
   {
    "duration": 27,
    "start_time": "2023-06-15T11:31:40.151Z"
   },
   {
    "duration": 671,
    "start_time": "2023-06-15T11:31:40.180Z"
   },
   {
    "duration": 1074381,
    "start_time": "2023-06-15T11:31:40.853Z"
   },
   {
    "duration": 6226,
    "start_time": "2023-06-15T11:49:35.235Z"
   },
   {
    "duration": 126798,
    "start_time": "2023-06-15T11:50:08.744Z"
   },
   {
    "duration": 13189,
    "start_time": "2023-06-15T11:53:42.836Z"
   },
   {
    "duration": 18667,
    "start_time": "2023-06-15T11:54:33.970Z"
   },
   {
    "duration": 380927,
    "start_time": "2023-06-15T11:54:57.351Z"
   },
   {
    "duration": 5082,
    "start_time": "2023-06-15T12:02:00.613Z"
   },
   {
    "duration": 969,
    "start_time": "2023-06-15T12:02:05.697Z"
   },
   {
    "duration": 10,
    "start_time": "2023-06-15T12:02:06.667Z"
   },
   {
    "duration": 39,
    "start_time": "2023-06-15T12:02:06.679Z"
   },
   {
    "duration": 551,
    "start_time": "2023-06-15T12:02:06.721Z"
   },
   {
    "duration": 1108451,
    "start_time": "2023-06-15T12:02:07.274Z"
   },
   {
    "duration": 6314,
    "start_time": "2023-06-15T12:20:35.732Z"
   },
   {
    "duration": 126484,
    "start_time": "2023-06-15T12:20:42.048Z"
   },
   {
    "duration": 13500,
    "start_time": "2023-06-15T12:22:48.534Z"
   },
   {
    "duration": 19239,
    "start_time": "2023-06-15T12:23:02.035Z"
   },
   {
    "duration": 2233692,
    "start_time": "2023-06-15T12:23:21.275Z"
   },
   {
    "duration": 185,
    "start_time": "2023-06-15T13:00:34.968Z"
   },
   {
    "duration": 9,
    "start_time": "2023-06-15T13:00:35.156Z"
   },
   {
    "duration": 222,
    "start_time": "2023-06-15T13:00:35.167Z"
   },
   {
    "duration": 2049792,
    "start_time": "2023-06-15T13:15:00.005Z"
   },
   {
    "duration": 167808,
    "start_time": "2023-06-15T13:56:26.904Z"
   },
   {
    "duration": 20,
    "start_time": "2023-06-15T13:59:34.355Z"
   },
   {
    "duration": 50,
    "start_time": "2023-06-15T14:00:29.474Z"
   },
   {
    "duration": 35,
    "start_time": "2023-06-15T14:00:38.926Z"
   },
   {
    "duration": 10,
    "start_time": "2023-06-15T14:01:23.840Z"
   },
   {
    "duration": 26,
    "start_time": "2023-06-15T14:01:31.537Z"
   },
   {
    "duration": 30,
    "start_time": "2023-06-15T14:02:28.105Z"
   },
   {
    "duration": 9,
    "start_time": "2023-06-15T14:03:57.369Z"
   },
   {
    "duration": 124733,
    "start_time": "2023-06-15T14:04:29.905Z"
   },
   {
    "duration": 19,
    "start_time": "2023-06-15T14:08:54.468Z"
   },
   {
    "duration": 11,
    "start_time": "2023-06-15T14:09:23.497Z"
   },
   {
    "duration": 121891,
    "start_time": "2023-06-15T14:09:30.053Z"
   },
   {
    "duration": 4985,
    "start_time": "2023-06-15T14:14:36.576Z"
   },
   {
    "duration": 6,
    "start_time": "2023-06-15T14:15:01.699Z"
   },
   {
    "duration": 11894,
    "start_time": "2023-06-15T14:15:14.277Z"
   },
   {
    "duration": 866,
    "start_time": "2023-06-15T14:15:26.173Z"
   },
   {
    "duration": 11,
    "start_time": "2023-06-15T14:15:27.041Z"
   },
   {
    "duration": 53,
    "start_time": "2023-06-15T14:15:27.053Z"
   },
   {
    "duration": 502,
    "start_time": "2023-06-15T14:15:27.108Z"
   },
   {
    "duration": 1129819,
    "start_time": "2023-06-15T14:15:27.612Z"
   },
   {
    "duration": 7023,
    "start_time": "2023-06-15T14:34:17.433Z"
   },
   {
    "duration": 124482,
    "start_time": "2023-06-15T14:34:24.458Z"
   },
   {
    "duration": 13559,
    "start_time": "2023-06-15T14:36:28.942Z"
   },
   {
    "duration": 19323,
    "start_time": "2023-06-15T14:36:42.503Z"
   },
   {
    "duration": 7540,
    "start_time": "2023-06-15T20:33:31.045Z"
   },
   {
    "duration": 2506,
    "start_time": "2023-06-15T20:33:38.587Z"
   },
   {
    "duration": 13,
    "start_time": "2023-06-15T20:33:41.095Z"
   },
   {
    "duration": 50,
    "start_time": "2023-06-15T20:33:41.109Z"
   },
   {
    "duration": 679,
    "start_time": "2023-06-15T20:33:41.161Z"
   },
   {
    "duration": 1281280,
    "start_time": "2023-06-15T20:33:41.842Z"
   },
   {
    "duration": 7602,
    "start_time": "2023-06-15T20:55:03.124Z"
   },
   {
    "duration": 118040,
    "start_time": "2023-06-15T20:55:10.728Z"
   },
   {
    "duration": 15477,
    "start_time": "2023-06-15T20:57:08.851Z"
   },
   {
    "duration": 23214,
    "start_time": "2023-06-15T20:57:24.330Z"
   },
   {
    "duration": 2400221,
    "start_time": "2023-06-15T20:57:47.546Z"
   },
   {
    "duration": 13,
    "start_time": "2023-06-15T21:37:47.769Z"
   },
   {
    "duration": 30,
    "start_time": "2023-06-15T21:37:47.784Z"
   },
   {
    "duration": 5987,
    "start_time": "2023-06-16T09:30:12.300Z"
   },
   {
    "duration": 3291,
    "start_time": "2023-06-16T09:30:18.290Z"
   },
   {
    "duration": 21,
    "start_time": "2023-06-16T09:30:21.583Z"
   },
   {
    "duration": 35,
    "start_time": "2023-06-16T09:30:21.607Z"
   },
   {
    "duration": 1087,
    "start_time": "2023-06-16T09:30:21.644Z"
   },
   {
    "duration": 1362362,
    "start_time": "2023-06-16T09:30:22.733Z"
   },
   {
    "duration": 7495,
    "start_time": "2023-06-16T09:53:05.097Z"
   },
   {
    "duration": 130572,
    "start_time": "2023-06-16T09:53:12.594Z"
   },
   {
    "duration": 15979,
    "start_time": "2023-06-16T09:55:23.171Z"
   },
   {
    "duration": 23076,
    "start_time": "2023-06-16T09:55:39.152Z"
   },
   {
    "duration": 2384176,
    "start_time": "2023-06-16T09:56:02.229Z"
   },
   {
    "duration": 8,
    "start_time": "2023-06-16T10:35:46.407Z"
   },
   {
    "duration": 13,
    "start_time": "2023-06-16T10:35:46.417Z"
   },
   {
    "duration": 17800,
    "start_time": "2023-06-16T10:50:13.993Z"
   },
   {
    "duration": 1018,
    "start_time": "2023-06-16T10:50:31.796Z"
   },
   {
    "duration": 15,
    "start_time": "2023-06-16T10:50:32.824Z"
   },
   {
    "duration": 53,
    "start_time": "2023-06-16T10:50:32.842Z"
   },
   {
    "duration": 860,
    "start_time": "2023-06-16T10:50:32.898Z"
   },
   {
    "duration": 1397510,
    "start_time": "2023-06-16T10:50:33.760Z"
   },
   {
    "duration": 8778,
    "start_time": "2023-06-16T11:13:51.275Z"
   },
   {
    "duration": 131830,
    "start_time": "2023-06-16T11:14:00.054Z"
   },
   {
    "duration": 16183,
    "start_time": "2023-06-16T11:16:11.889Z"
   },
   {
    "duration": 25319,
    "start_time": "2023-06-16T11:16:28.075Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
